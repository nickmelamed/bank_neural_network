{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba652776-bb62-4ca1-a82d-7184ca6cae1f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc5f9f5-8da0-42ce-b469-34cc624a9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim \n",
    "from imblearn import over_sampling\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import hyperparam_tuning as ht\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a78dce-0ee6-49d2-925d-54e8eb42618b",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303f6020-c87d-4e56-81ce-78a4077c400b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/bank-additional-full.csv\", delimiter = \";\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d234e-ba62-467f-a1f6-276ecbdb7397",
   "metadata": {},
   "source": [
    "The description of the columns can be found at the UCI Machine Learning Repository, linked [here](https://archive.ics.uci.edu/dataset/222/bank+marketing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9cdd6e-9607-428e-829a-f195b2803c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
       "       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n",
       "       'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
       "       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de3c15-b5b2-4671-82d6-93fb532c83b1",
   "metadata": {},
   "source": [
    "For this particular analysis, I am going to ignore economic factors, as I will take the simplifying assumption that the majority of people who are in this campaign are not making decisions based on economic factors like the consumer price index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce26f816-4819-4add-bf86-73467efec00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  duration  campaign  pdays  previous     poutcome   y  \n",
       "0   may         mon       261         1    999         0  nonexistent  no  \n",
       "1   may         mon       149         1    999         0  nonexistent  no  \n",
       "2   may         mon       226         1    999         0  nonexistent  no  \n",
       "3   may         mon       151         1    999         0  nonexistent  no  \n",
       "4   may         mon       307         1    999         0  nonexistent  no  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.columns[~df.columns.isin(['emp.var.rate', 'cons.price.idx','cons.conf.idx', 'euribor3m', 'nr.employed'])]]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf261b37-5288-480a-bc99-04bb9a0afe8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no         32588\n",
       "unknown     8597\n",
       "yes            3\n",
       "Name: default, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7808cb1a-d7f8-4908-ba4f-c68913ee27db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes        21576\n",
       "no         18622\n",
       "unknown      990\n",
       "Name: housing, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['housing'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b452bb3-d02b-4a40-8c20-e2438ac5c482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no         33950\n",
       "yes         6248\n",
       "unknown      990\n",
       "Name: loan, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['loan'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9036b73-5650-4e4a-84ef-3c05b73f5dff",
   "metadata": {},
   "source": [
    "As we can see from above, the number of customers who have defaulted on previous loans is relatively low. So, to avoid dealing with the unknown values, we will make the assumption that the vast majority of people have not defaulted and therefore this column does not provide us any predictive information. \n",
    "\n",
    "We will also drop the unknowns from people who have taken housing or personal loans, as we have a sufficient sample size without the unknowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07781a83-1e03-4e5c-93bb-216d46aa5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['default'])\n",
    "\n",
    "df = df[(df['housing'] != 'unknown') & (df['loan'] != 'unknown')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5e3f1-68b0-431a-9400-b0f66dc1cd56",
   "metadata": {},
   "source": [
    "In order to run our neural network, we have to make all of our variables numeric. We do so by turning any categorical variables into dummy variables (1 = variable is true, 0 = false), and, since the binary variables are stored as yes/no variables, we will convert those into 1/0 values as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6da9a5b-2421-40e7-a67b-e25a9de72830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>y</th>\n",
       "      <th>job: admin.</th>\n",
       "      <th>job: blue-collar</th>\n",
       "      <th>...</th>\n",
       "      <th>month: oct</th>\n",
       "      <th>month: sep</th>\n",
       "      <th>day_of_week: fri</th>\n",
       "      <th>day_of_week: mon</th>\n",
       "      <th>day_of_week: thu</th>\n",
       "      <th>day_of_week: tue</th>\n",
       "      <th>day_of_week: wed</th>\n",
       "      <th>poutcome: failure</th>\n",
       "      <th>poutcome: nonexistent</th>\n",
       "      <th>poutcome: success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  housing  loan  duration  campaign  pdays  previous  y  job: admin.  \\\n",
       "0   56        0     0       261         1    999         0  0            0   \n",
       "1   57        0     0       149         1    999         0  0            0   \n",
       "2   37        1     0       226         1    999         0  0            0   \n",
       "3   40        0     0       151         1    999         0  0            1   \n",
       "4   56        0     1       307         1    999         0  0            0   \n",
       "\n",
       "   job: blue-collar  ...  month: oct  month: sep  day_of_week: fri  \\\n",
       "0                 0  ...           0           0                 0   \n",
       "1                 0  ...           0           0                 0   \n",
       "2                 0  ...           0           0                 0   \n",
       "3                 0  ...           0           0                 0   \n",
       "4                 0  ...           0           0                 0   \n",
       "\n",
       "   day_of_week: mon  day_of_week: thu  day_of_week: tue  day_of_week: wed  \\\n",
       "0                 1                 0                 0                 0   \n",
       "1                 1                 0                 0                 0   \n",
       "2                 1                 0                 0                 0   \n",
       "3                 1                 0                 0                 0   \n",
       "4                 1                 0                 0                 0   \n",
       "\n",
       "   poutcome: failure  poutcome: nonexistent  poutcome: success  \n",
       "0                  0                      1                  0  \n",
       "1                  0                      1                  0  \n",
       "2                  0                      1                  0  \n",
       "3                  0                      1                  0  \n",
       "4                  0                      1                  0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting categorical variables into dummies\n",
    "\n",
    "df_dummy = pd.get_dummies(df, columns = ['job', 'marital', 'education', 'contact', 'month', 'day_of_week', 'poutcome'], prefix_sep = ': ')\n",
    "\n",
    "# turning y/n variables into 1/0\n",
    "\n",
    "df_dummy['housing'] = np.where(df['housing'].values == 'yes', 1, 0)\n",
    "df_dummy['loan'] = np.where(df['loan'].values == 'yes', 1, 0)\n",
    "df_dummy['y'] = np.where(df['y'].values == 'yes', 1, 0)\n",
    "\n",
    "df_dummy.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5547b18-d530-4dbc-8968-13f77f852ad8",
   "metadata": {},
   "source": [
    "## Building the Basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd5c7e-1166-4d4b-a45c-adb4a6e2e8f3",
   "metadata": {},
   "source": [
    "For any model building, we want to ensure our model is not trained on the entirety of our dataset. Otherwise, we would have no data to test the model on, and run the risk of overtraining. I have decided to use Scikit-Learn to split the dataset into 70% training data, 30% testing data. An 80-20 split is another popular option but I prefer to have more testing data to ensure the model is even less susceptible to the overtraining problem.\n",
    "\n",
    "We make sure our outcome variable, y (whether or not the client subscribed to the term deposit, the goal of the campaign), is separated from our other predictor variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "139b4ff9-3256-4f47-961d-7b8f247a4e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dummy[df_dummy.columns[~df_dummy.columns.isin(['y'])]]\n",
    "y = df_dummy['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932eacd3-4074-4d79-b3e9-e3847942e8c1",
   "metadata": {},
   "source": [
    "### Training the Sequential Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021dffef-232c-4c4e-b3cd-7666953e4a1e",
   "metadata": {},
   "source": [
    "For our model, we will be using PyTorch's Sequential Neural Network (SNN), as it allows us to utilize multiple layers in a sequential order. Being able to apply multiple activation functions to our model allows for more thorough training of the model. \n",
    "\n",
    "For this given model, we use two Linear Modules as our hidden layers to perform linear transformations (for ease of calculation), along with the ReLU activation function for both layers. Our output layer makes use of a Sigmoid function, as this maps our transformed data to [0,1] for classification. \n",
    "\n",
    "With regards to the number of neurons we use for our hidden layers, many rules of thumb have been proposed. Some have suggested that the number of neurons should be $\\frac{2}{3}$ that of the output layer, some have suggested no more than 2x the input layer. We will use K-fold Cross Validation to determine which option, including a 3rd possibility of the middle of the road of the two (~1.3x input layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ab26524-4bb4-4404-9e94-2c73a25d11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert sequential_NN model here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d3057-e25f-4ecf-9ebf-70b940c5e1cc",
   "metadata": {},
   "source": [
    "Since we are using a binary classifier (either the campaign is successful or it isn't), we use Binary Cross Entropy loss as it measures the error in mislabeled outcomes for a single outcome vector. We use the Adam optimizer due to its ability to have quick convergence and deal with sparse gradients, which we may deal with as a result of having a lot of dummy variables. \n",
    "\n",
    "We are going to train the model via mini-batch Stochasic Gradient Descent (SGD), meaning we have to select a learning rate, a batch size, and a number of epochs. \n",
    "\n",
    "The learning rate is how big of a step we want to take in our gradient calculations, and while 0.001 is considered the default for the Adam Optimizer, we would like to see if a different learning rate, such as 0.01 or 0.0001, would be more optimal in terms of minimizing model loss. \n",
    "\n",
    "For batch size, or the amount of data we are going to train at a time, 32 is common, but higher powers of 2, such as 64 or 128, may yield better results. \n",
    "\n",
    "For epochs, or the number of times the entirety of our data is trained, 100 is a common number, but we will experiment with a smaller epoch number like 50, or a larger one like 200, to see what a more optimal number is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d23459b-2b03-4915-a1fb-ff10d5692cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traing sgd goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50044d2f-c905-4972-b867-66fad9f7b74a",
   "metadata": {},
   "source": [
    "### K-Cross Fold Validation for Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ca96c-e05c-49f5-8531-e58e7a1d111f",
   "metadata": {},
   "source": [
    "As mentioned above, we have a few hyperparameters that we need to select in order to create and train our model. We will go about selecting these via K-Cross Fold Validation, which looks something like this:\n",
    "\n",
    "For each combination of hyperparameters: <br>\n",
    "    Split data into k folds. <br><br>\n",
    "        For each fold: <br>\n",
    "            Use fold as validation set, other k-1 folds as validation set. <br>\n",
    "            Train model with combination of hyperparameters on training data, validate it on validation set. <br>\n",
    "            Calculate loss. <br><br>\n",
    "    Average validation loss (known as cross validation error) across folds. <br>\n",
    "    \n",
    "We select the combination of hyperparameters that minimizes this cross validation loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528cbfb-20b4-4785-8a99-be95d54a3832",
   "metadata": {},
   "source": [
    "We list our aforementioned hyperparameters below, and will run our K-Cross Fold Validation on these combinations. \n",
    "\n",
    "As for the number of folds, we already are testing a lot of different parameters, so for the sake of time, we will use a common default k = 5 folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7b139-b884-4ccb-a07f-d75e340f215e",
   "metadata": {},
   "source": [
    "We also define a function that will take our validation folds, use our model to generate predictions, and return the predicted probabilities so that we may properly evaluate BCE loss for the validation set. \n",
    "\n",
    "We then define our own BCE loss function so that we may calculate the BCE loss between our model's classifications and the true classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40ffb52e-08db-4b73-9b40-08703a351a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_prob and bce go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dc82d1e-6294-49ab-ba94-4338d68c6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to tensor goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b682a8-ac5b-4bdb-ac7b-5dba9a2ef469",
   "metadata": {},
   "source": [
    "Note I make use of multiprocessing in order to speed up this computation, as it takes quite long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd4e94f9-1bf1-4984-8ce1-7871bd821164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hidden Neurons': 2, 'Learning Rate': 0.01, 'Batch Size': 64, 'Epochs': 50}\n"
     ]
    }
   ],
   "source": [
    "# cross validation and best hyperparams go here\n",
    "\n",
    "hidden_neurons = [2/3, (2 + 2/3)/2, 2]\n",
    "alphas = [0.0001, 0.001, 0.01]\n",
    "batches = [32, 64, 128]\n",
    "epochs = [50, 100, 200]\n",
    "\n",
    "optimal_params = ht.best_hyperparameters(hidden_neurons, alphas, batches, epochs, X, y)\n",
    "print(optimal_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46edff7-24f4-4a3c-934a-4c7a59c4c09f",
   "metadata": {},
   "source": [
    "Based on our above output, our model that has the smallest cross validation error is the one with the following hyperparameters:\n",
    "    \n",
    "Hidden neurons: 2 * Input Neurons <br><br>\n",
    "Learning rate: 0.01 <br><br>\n",
    "Batch Size: 64 <br><br>\n",
    "Epochs: 50 <br><br>\n",
    "\n",
    "So we will use these hyperparameters to train our model, and see what our performance looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76f5de5b-e7ba-4e83-a57d-e7512cabd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .70)\n",
    "\n",
    "X_train_tensor = ht.df_to_tensor(X_train, outcome = False) # convert to tensors for model processing\n",
    "y_train_tensor = ht.df_to_tensor(y_train, outcome = True)\n",
    "\n",
    "neural = ht.sequential_NN(2) # create NN with given hidden neurons \n",
    "\n",
    "# training model with optimal hyperparameters\n",
    "\n",
    "ht.train_sgd(lr = 0.01, model = neural, X = X_train_tensor, y = y_train_tensor, num_epochs = 50, batch_size = 64) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad09cd-3bed-4553-9875-5665b84b2426",
   "metadata": {},
   "source": [
    "### Training Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba24e94-a394-4494-99fa-43e9994002a2",
   "metadata": {},
   "source": [
    "We compute the training accuracy of our model, which is the percentage of correctly labelled outcomes, to determine how well it classifies our desired outcome.\n",
    "\n",
    "Note we also create a function tha turns our predicted probabilities into actual class predictions so we may calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b689a067-7121-4dac-9fd6-c2e8b4a8cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_prob(probabilities):\n",
    "    ''' Given the predicted probabilities, returns the predicted classes\n",
    "    \n",
    "        Inputs:\n",
    "            probabilities (tensor): tensor containing predicted probabilities for X (output from ht.pred_prob)\n",
    "            \n",
    "        Outputs:\n",
    "            classifications (tensor): tensor containing the class predictions for X \n",
    "            \n",
    "    '''\n",
    "    classifications = probabilities.round() # probabilities >= 0.5 get rounded to 1, under to 0 \n",
    "    return classifications\n",
    "\n",
    "\n",
    "def accuracy(classifications, y):\n",
    "    ''' Calculate the accuracy, or the percentage of correct predictions, of the model \n",
    "    \n",
    "        Inputs:\n",
    "            classifications (tensor): tensor of predicted classes (output of classify_data)\n",
    "            y (tensor): tensor of true classes\n",
    "        \n",
    "        Outputs: \n",
    "            accuracy (float): accuracy of the predictions as a percentage\n",
    "        \n",
    "    '''\n",
    "    accuracy = (classifications == y).float().mean()\n",
    "    return round(float(accuracy)*100, 2) # have to convert accuracy from Tensor to float to convert as percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5859a00-d9bd-4379-9450-f8cf975a8721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 89.65%\n"
     ]
    }
   ],
   "source": [
    "train_probabilities = ht.pred_prob(X_train_tensor, neural)\n",
    "train_classifications = classify_prob(train_probabilities)\n",
    "    \n",
    "train_accuracy = accuracy(train_classifications, y_train_tensor)\n",
    "print(f\"Training Accuracy: {train_accuracy}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca026bf8-2d21-4743-aa74-92d071f9633a",
   "metadata": {},
   "source": [
    "This is a very encouraging training accuracy! However, it is quite high and could be a result of overtraining our data, or it could be the result of us having a class imbalance.\n",
    "\n",
    "Let us examine the class imbalance possibility: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bf3b83a4-9815-4832-9fe9-c719ffa76528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positives: 4533\n",
      "Number of Negatives: 35665\n",
      "Ratio of Positives to Negatives: 0.13\n"
     ]
    }
   ],
   "source": [
    "num_pos = sum(y) # we are dealing with 0s and 1s!\n",
    "num_neg = len(y) - sum(y)\n",
    "print(f\"Number of Positives: {num_pos}\")\n",
    "print(f\"Number of Negatives: {num_neg}\")\n",
    "print(f\"Ratio of Positives to Negatives: {round(num_pos / num_neg, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec99b4f-feb7-4398-989e-c641d25a5444",
   "metadata": {},
   "source": [
    "We do have a rather large imbalance in class size, which could be artificially inflating our accuracy measure. Considering the size imbalance is large, but not to the point where we have too small of a sample size for either class, we will proceed forward with some other metrics to gain more insight into our model's performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bd8ee905-61b4-4c2d-afec-acc83630b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definining functions to calculate performance metrics \n",
    "\n",
    "def calculate_pos_neg(classifications, original):\n",
    "    ''' Calculates TP, TN, FP, FN of given Tensors\n",
    "    \n",
    "        Inputs:\n",
    "            classifications (tensor): tensor of predicted classes (output of classify_prob)\n",
    "            original (tensor): original outcome/class assignments\n",
    "        \n",
    "        Outputs:\n",
    "            results (list): List of TP, TN, FP, and FN \n",
    "    '''\n",
    "    \n",
    "    combined = torch.stack((classifications, original), 0) # combined[0] is classifications, combined[1] is y_tensor\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    for i in range(len(classifications)):\n",
    "        if combined[0][i] == combined[1][i]:\n",
    "            if combined[0][i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        if combined[0][i] != combined[1][i]:\n",
    "            if combined[0][i] == 1:\n",
    "                fp += 1\n",
    "            else: \n",
    "                fn += 1\n",
    "                \n",
    "    return [tp, fp, tn, fn]\n",
    "\n",
    "def calculate_metrics(frequencies):\n",
    "    ''' Calculates Precision, Recall, F1 Score, Specificity, True Negative Rate, and False Negative Rate\n",
    "    \n",
    "        Inputs:\n",
    "            frequencies (list): List consisting of TP, TN, FP, FN (all integers, outputted from calcualte_pos_neg)\n",
    "        \n",
    "        Outputs:\n",
    "            results (list): List of Precision, Recall, F1 Score, Specificity, True Negative Rate, and False Negative Rate\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    tp = frequencies[0]\n",
    "    fp = frequencies[1]\n",
    "    tn = frequencies[2]\n",
    "    fn = frequencies[3]\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    specificity = fp / (fp + tn)\n",
    "    tnr = 1 - specificity # TNR and Specificity are complements \n",
    "    fnr = 1 - recall # FNR and Recall are complements \n",
    "    \n",
    "    return [precision, recall, f1, specificity, tnr, fnr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6b11998-dcf0-4f18-8800-9a6d44f77c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision: 53.2%\n",
      "Training Recall: 62.1%\n",
      "Training F1 Score: 0.57\n",
      "Training Specificity: 6.88%\n",
      "Training True Negative Rate (TNR): 93.12%\n",
      "Training False Negative Rate (FNR): 37.9%\n"
     ]
    }
   ],
   "source": [
    "training_metrics = calculate_metrics(calculate_pos_neg(train_classifications, y_train_tensor))\n",
    "\n",
    "\n",
    "print(f\"Training Precision: {round(training_metrics[0] * 100, 2)}%\")\n",
    "print(f\"Training Recall: {round(training_metrics[1] * 100, 2)}%\")\n",
    "print(f\"Training F1 Score: {round(training_metrics[2], 2)}\")\n",
    "print(f\"Training Specificity: {round(training_metrics[3] * 100, 2)}%\")\n",
    "print(f\"Training True Negative Rate (TNR): {round(training_metrics[4] * 100, 2)}%\")\n",
    "print(f\"Training False Negative Rate (FNR): {round(training_metrics[5] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2416ec-59e0-4165-8116-a08ecaf4f230",
   "metadata": {},
   "source": [
    "As we can see, with these other metrics, our model's accuracy is not reflective of what we would want to see. The training model is exceptionally good at identifying negatives correctly (TNR), but only decent at identifying positives correctly (Precision). \n",
    "\n",
    "Our F1 score, which is the weighted mean of precision and recall, suggests that our model is average at best at predicting positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0f761-2e41-4a00-9c4e-c69551b0c448",
   "metadata": {},
   "source": [
    "### Fitting Model on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686bf440-cb75-45eb-9457-54f9beee6815",
   "metadata": {},
   "source": [
    "To address the overfitting concern, let us use our test data to assess the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c156a2a6-deb3-489f-adb7-b55252519cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting test data to Tensors\n",
    "\n",
    "X_test_tensor = ht.df_to_tensor(X_test, outcome = False)\n",
    "y_test_tensor = ht.df_to_tensor(y_test, outcome = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9b37ef33-ad2a-4700-9185-79b52421dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.88%\n"
     ]
    }
   ],
   "source": [
    "# computing test accuracy \n",
    "\n",
    "test_probabilities = ht.pred_prob(X_test_tensor, neural)\n",
    "test_classifications = classify_prob(test_probabilities)\n",
    "    \n",
    "test_accuracy = accuracy(test_classifications, y_test_tensor)\n",
    "print(f\"Test Accuracy: {test_accuracy}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce464b40-9cc4-43e6-912e-eee54cf8d1e5",
   "metadata": {},
   "source": [
    "We have a similarly high test data accuracy, so let's see if our imbalance problems carry over: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44017008-13db-47c7-965c-2122d455ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Precision: 55.32%\n",
      "Testing Recall: 61.52%\n",
      "Testing F1 Score: 0.58\n",
      "Testing Specificity: 6.44%\n",
      "Testing True Negative Rate (TNR): 93.56%\n",
      "Testing False Negative Rate (FNR): 38.48%\n"
     ]
    }
   ],
   "source": [
    "testing_metrics = calculate_metrics(calculate_pos_neg(test_classifications, y_test_tensor))\n",
    "\n",
    "\n",
    "print(f\"Testing Precision: {round(testing_metrics[0] * 100, 2)}%\")\n",
    "print(f\"Testing Recall: {round(testing_metrics[1] * 100, 2)}%\")\n",
    "print(f\"Testing F1 Score: {round(testing_metrics[2], 2)}\")\n",
    "print(f\"Testing Specificity: {round(testing_metrics[3] * 100, 2)}%\")\n",
    "print(f\"Testing True Negative Rate (TNR): {round(testing_metrics[4] * 100, 2)}%\")\n",
    "print(f\"Testing False Negative Rate (FNR): {round(testing_metrics[5] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefede8-de1a-4ac9-bd93-54c9965eff72",
   "metadata": {},
   "source": [
    "We perform roughly the same compared to our training model. \n",
    "\n",
    "Is there a way for us to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66c544-0a02-492b-9f9a-a44fd47e38fc",
   "metadata": {},
   "source": [
    "## Solution 1: Fixing the Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c0db2-ae9f-403e-8c73-35033bba83bb",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98a1fc-4b00-470f-b0a7-25b9ec74e1a1",
   "metadata": {},
   "source": [
    "SMOTE, or Synthetic Minority Oversampling Technique, is a class imbalance correction technique. SMOTE works to correct class imbalance by synthetically creating new minority class datapoints, which avoids the hiccups involved with random techniques like over or undersampling. \n",
    "\n",
    "There are some limitations, like trouble translating to higher dimensions, or the possibility of adding noise to the data because it does not consider the majority class when creating synthetic minority class data points.\n",
    "\n",
    "First, let's generate the new, more balanced classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3f34c15e-3679-46ba-862d-5d2873c56bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Number of Positives: 4533 vs. New Number of Positives: 10699\n",
      "Original Number of Negatives: 35665 vs. New Number of Negatives: 35665\n"
     ]
    }
   ],
   "source": [
    "oversample = over_sampling.SMOTE(sampling_strategy = 0.3)\n",
    "X_bal, y_bal = oversample.fit_resample(X, y)\n",
    "\n",
    "bal_num_pos = sum(y_bal)\n",
    "bal_num_neg = len(y_bal) - bal_num_pos\n",
    "\n",
    "print(f\"Original Number of Positives: {num_pos} vs. New Number of Positives: {bal_num_pos}\")\n",
    "print(f\"Original Number of Negatives: {num_neg} vs. New Number of Negatives: {bal_num_neg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45437d1b-b052-4f32-8426-748dd752f760",
   "metadata": {},
   "source": [
    "All we have done is inflate the number of positives so that they now constitute roughly 30% of our dataset. Let us see if this creates any improvement in model performance: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623461b3-9a74-47f2-9571-6135cb658957",
   "metadata": {},
   "source": [
    "### Training Model Post-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bc6a8f0e-6411-41db-8157-d6bcd7dbc304",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bal_train, X_bal_test, y_bal_train, y_bal_test = train_test_split(X_bal, y_bal, random_state=0, train_size = .70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9281d598-f243-4a18-bc0e-397394c63d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting balanced training data into tensors\n",
    "\n",
    "X_bal_train_tensor = ht.df_to_tensor(X_bal_train, outcome = False)\n",
    "y_bal_train_tensor = ht.df_to_tensor(y_bal_train, outcome = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9753f920-c5af-4be3-89b7-e180062836a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht.train_sgd(lr = 0.01, model = neural, X = X_bal_train_tensor, y = y_bal_train_tensor, num_epochs = 50, batch_size = 64) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82192d45-9a34-4d2d-af3b-ecc6f65d5597",
   "metadata": {},
   "source": [
    "### Training Performance w/ SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "40483105-f880-4b63-ae10-cbb17114ab82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Training Accuracy: 91.36%\n"
     ]
    }
   ],
   "source": [
    "bal_train_probabilities = ht.pred_prob(X_bal_train_tensor, neural)\n",
    "bal_train_classifications = classify_prob(bal_train_probabilities)\n",
    "    \n",
    "bal_train_accuracy = accuracy(bal_train_classifications, y_bal_train_tensor)\n",
    "print(f\"Balanced Training Accuracy: {bal_train_accuracy}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c7f1b-266b-4d4d-af61-794555653ec8",
   "metadata": {},
   "source": [
    "We observe a similar level of accuracy to our previous attempts, but given our much less severe class imbalance, I would believe this will result in better performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "25468f76-0191-45ed-9db5-474bc3ae39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Training Precision: 79.78%\n",
      "Balanced Training Recall: 83.74%\n",
      "Balanced Training F1 Score: 0.82\n",
      "Balanced Training Specificity: 6.36%\n",
      "Balanced Training True Negative Rate (TNR): 93.64%\n",
      "Balanced Training False Negative Rate (FNR): 16.26%\n"
     ]
    }
   ],
   "source": [
    "bal_training_metrics = calculate_metrics(calculate_pos_neg(bal_train_classifications, y_bal_train_tensor))\n",
    "\n",
    "\n",
    "print(f\"Balanced Training Precision: {round(bal_training_metrics[0] * 100, 2)}%\")\n",
    "print(f\"Balanced Training Recall: {round(bal_training_metrics[1] * 100, 2)}%\")\n",
    "print(f\"Balanced Training F1 Score: {round(bal_training_metrics[2], 2)}\")\n",
    "print(f\"Balanced Training Specificity: {round(bal_training_metrics[3] * 100, 2)}%\")\n",
    "print(f\"Balanced Training True Negative Rate (TNR): {round(bal_training_metrics[4] * 100, 2)}%\")\n",
    "print(f\"Balanced Training False Negative Rate (FNR): {round(bal_training_metrics[5] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bf740-7ab0-408e-82fa-dc21deb4d339",
   "metadata": {},
   "source": [
    "We see a dramatic improvement in our metrics! We have a very strong positive predictor now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848e1f6-9abb-499b-9ade-33122764ec0d",
   "metadata": {},
   "source": [
    "Let's make sure this carries over to our testing data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb31003-ff63-4c3d-aec7-37ec993d6743",
   "metadata": {},
   "source": [
    "### Testing Model Post-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "98bd44bc-a621-4d4e-b8ea-1411125cbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bal_test_tensor = ht.df_to_tensor(X_bal_test, outcome = False)\n",
    "y_bal_test_tensor = ht.df_to_tensor(y_bal_test, outcome = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "561c2188-9165-4453-a973-b2a8e8c97f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Test Accuracy: 91.01%\n"
     ]
    }
   ],
   "source": [
    "bal_test_probabilities = ht.pred_prob(X_bal_test_tensor, neural)\n",
    "bal_test_classifications = classify_prob(bal_test_probabilities)\n",
    "    \n",
    "bal_test_accuracy = accuracy(bal_test_classifications, y_bal_test_tensor)\n",
    "print(f\"Balanced Test Accuracy: {bal_test_accuracy}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a04ce5-6fb5-4c30-a339-89617115a897",
   "metadata": {},
   "source": [
    "We have a similarly high training accuracy, but let's see if this carries over to the other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b9652afb-657c-4250-9cfb-0e4b9199a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Test Precision: 79.1%\n",
      "Balanced Test Recall: 83.1%\n",
      "Balanced Test F1 Score: 0.81\n",
      "Balanced Test Specificity: 6.61%\n",
      "Balanced Test True Negative Rate (TNR): 93.39%\n",
      "Balanced Test False Negative Rate (FNR): 16.9%\n"
     ]
    }
   ],
   "source": [
    "bal_testing_metrics = calculate_metrics(calculate_pos_neg(bal_test_classifications, y_bal_test_tensor))\n",
    "\n",
    "\n",
    "print(f\"Balanced Test Precision: {round(bal_testing_metrics[0] * 100, 2)}%\")\n",
    "print(f\"Balanced Test Recall: {round(bal_testing_metrics[1] * 100, 2)}%\")\n",
    "print(f\"Balanced Test F1 Score: {round(bal_testing_metrics[2], 2)}\")\n",
    "print(f\"Balanced Test Specificity: {round(bal_testing_metrics[3] * 100, 2)}%\")\n",
    "print(f\"Balanced Test True Negative Rate (TNR): {round(bal_testing_metrics[4] * 100, 2)}%\")\n",
    "print(f\"Balanced Test False Negative Rate (FNR): {round(bal_testing_metrics[5] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853a338-a752-40be-8a72-d84c14ec62ec",
   "metadata": {},
   "source": [
    "Our performance on the test set is roughly the same as our training data! We have a strong classifier on our hands! \n",
    "\n",
    "Can we make it even better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5a2c1-f536-45bf-9067-1836c3206bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
